<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<configuration>
  <property>
    <name>logfeeder.cloud.storage.mode</name>
    <value>default</value>
    <description>Option to support sending logs to cloud storage. You can choose between supporting only cloud storage, non-cloud storage or both</description>
    <display-name>Log Feeder Mode</display-name>
    <value-attributes>
      <type>value-list</type>
      <entries>
        <entry>
          <value>default</value>
          <label>DEFAULT</label>
        </entry>
        <entry>
          <value>cloud</value>
          <label>CLOUD</label>
        </entry>
        <entry>
          <value>hybrid</value>
          <label>HYBRID</label>
        </entry>
      </entries>
      <selection-cardinality>1</selection-cardinality>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.storage.base.path</name>
    <value>/apps/logsearch</value>
    <description>Base path prefix for storing logs (on cloud storage / hdfs), can be URI as well (like s3a://) - that is only used if Cloud mode is enabled for Log Feeder</description>
    <display-name>Storage Base Path</display-name>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.storage.use.filters</name>
    <value>false</value>
    <description>Use filters for inputs (with filters the output format will be JSON). It can slow down the log processing, so it is not recommended to use in hybrid mode.</description>
    <display-name>Use Filters</display-name>
    <value-attributes>
      <type>value-list</type>
      <entries>
        <entry>
          <value>true</value>
          <label>ON</label>
        </entry>
        <entry>
          <value>false</value>
          <label>OFF</label>
        </entry>
      </entries>
      <selection-cardinality>1</selection-cardinality>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.storage.custom.fs</name>
    <value/>
    <display-name>Custom Default FS</display-name>
    <description>If it is not empty, override fs.defaultFS for HDFS client. Can be useful to write data to a different bucket (from other services) if the bucket address is read from core-site.xml (e.g.: s3a://myotherpath)
    </description>
    <value-attributes>
      <empty-value-valid>true</empty-value-valid>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.hdfs.file.permissions</name>
    <value>640</value>
    <display-name>HDFS Default FS Permissions</display-name>
    <description>
      Default file permissions for newly created files on HDFS by Log Feeder
    </description>
    <value-attributes>
      <type>int</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.hdfs.user</name>
    <value>{logsearch_user}</value>
    <display-name>Log Feeder HDFS Alias</display-name>
    <description>
      User that is used on HDFS for Log Feeder. Only used on non-secure env. (Set this only if simple HDFS is used as Cloud Storage, as probably the Log Feeder user (root or ambari-agent user) won't exists on HDFS)
    </description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.config.files</name>
    <value>{default_config_files}</value>
    <description>Comma separated config files in grok format</description>
    <display-name>Log Feeder Config Files</display-name>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.checkpoint.folder</name>
    <value>/usr/lib/ambari-logsearch-logfeeder/conf/checkpoints</value>
    <description>Checkpoint folder for Log Feeder</description>
    <display-name>Log Feeder Checkpoint Dir</display-name>
    <value-attributes>
      <type>directory</type>
    </value-attributes>
    <on-ambari-upgrade add="true" update="true"/>
  </property>
  <property>
    <name>logfeeder.metrics.collector.hosts</name>
    <value>{metrics_collector_hosts}</value>
    <description>Metrics collector hosts for pushing metrics by Log Feeder</description>
    <display-name>Metrics Collector Hosts</display-name>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.log.filter.enable</name>
    <value>true</value>
    <description>Enable Log filtering based on log level (INFO,ERROR,WARN etc.)</description>
    <display-name>Log Filter Enabled</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.configs.filter.solr.monitor.enabled</name>
    <value>true</value>
    <description>Enable to monitor log level filters (in solr) periodically - used for checking updates. If it is disabled - after changing a filer - restarting Log Feeders are required.</description>
    <display-name>Monitor Filter Changes</display-name>
    <value-attributes>
      <type>value-list</type>
      <overridable>false</overridable>
      <entries>
        <entry>
          <value>true</value>
          <label>ON</label>
        </entry>
        <entry>
          <value>false</value>
          <label>OFF</label>
        </entry>
      </entries>
      <selection-cardinality>1</selection-cardinality>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.configs.filter.solr.monitor.interval</name>
    <value>30</value>
    <description>Interval to fetch filter config from Solr in seconds</description>
    <display-name>Fetch Filters Interval</display-name>
    <value-attributes>
      <minimum>5</minimum>
      <maximum>300</maximum>
      <increment-step>1</increment-step>
      <type>seconds</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.solr.implicit.routing</name>
    <value>false</value>
    <description>
      Use implicit routing for Solr Collections.
    </description>
    <display-name>Log Feeder Solr Implicit Routing</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cache.enabled</name>
    <value>false</value>
    <description>
      Enable input cache for every monitored input file. The cache stores log lines, based on the data, duplications can be dropped.
    </description>
    <display-name>Input Cache Enabled</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cache.size</name>
    <value>100</value>
    <description>Size of the input caches</description>
    <display-name>Input Cache Size</display-name>
    <value-attributes>
      <type>int</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cache.dedup.interval</name>
    <value>1000</value>
    <description>
      If input cache is enabled, Log Feeder can drop any duplicated line during log processing,
      but only if the duplicated lines/messages are in the same interval (in milliseconds) with the original message/line.
    </description>
    <display-name>Input Cache Dedup Interval</display-name>
    <value-attributes>
      <type>int</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cache.last.dedup.enabled</name>
    <value>false</value>
    <description>
      If last dedup is enabled for input cache, Log Feeder will drop every new line (message), which is the same as the last line.
    </description>
    <display-name>Input Cache Last Dedup</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cache.key.field</name>
    <value>log_message</value>
    <description>
      Key field, which will be used as keys in the Input cache. (by defalt, log_message represets the message part of processed data)
    </description>
    <display-name>Input Cache Key Field</display-name>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.include.default.level</name>
    <value>FATAL,ERROR,WARN</value>
    <description>Include default Log Feeder Log Levels for Log Search. Used for only bootstrapping the configuration if shared log level filters are disabled. (levels: FATAL,ERROR,WARN,INFO,DEBUG,TRACE,UNKNOWN)</description>
    <display-name>Log Feeder Default Log Levels</display-name>
    <on-ambari-upgrade add="true"/>
  </property>

  <property>
    <name>logfeeder.cloud.rollover.archive.base.dir</name>
    <value>/var/lib/ambari-logsearch-logfeeder/data</value>
    <description>
      Location where the active and archives logs will be stored. Beware, it could require a large amount of space, use mounted disks if it is possible.
    </description>
    <display-name>Log Archive Base Dir</display-name>
    <value-attributes>
      <type>directory</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.threshold.min</name>
    <value>120</value>
    <description>
      Rollover cloud log files after an interval (minutes)
    </description>
    <display-name>Log Threshold Time</display-name>
    <value-attributes>
      <minimum>10</minimum>
      <maximum>1440</maximum>
      <increment-step>10</increment-step>
      <type>minutes</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.threshold.size</name>
    <value>80</value>
    <description>
      Rollover cloud log files after the log file size reach this limit
    </description>
    <display-name>Log Threshold Size</display-name>
    <value-attributes>
      <type>int</type>
      <minimum>1</minimum>
      <maximum>200</maximum>
      <unit>MB</unit>
      <increment-step>1</increment-step>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.max.backup.files</name>
    <value>10</value>
    <description>
      The maximum number of backup files for rollover cloud logs.
    </description>
    <display-name>Log Max Files</display-name>
    <value-attributes>
      <type>int</type>
      <minimum>1</minimum>
      <maximum>20</maximum>
      <increment-step>1</increment-step>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.immediate.flush</name>
    <value>true</value>
    <description>
      Immediately flush temporal cloud logs (to active location) - avoiding the flush operation at the end of each append results in a performance gain of 10 to 20 percent.
      However, there is safety trade-off involved in skipping flushing. Indeed, when flushing is skipped, then it is likely that the last few log events will not be recorded on disk when the application exits.
      This is a high price to pay even for a 20% performance gain.
    </description>
    <display-name>Immediate Flush</display-name>
    <value-attributes>
      <type>value-list</type>
      <entries>
        <entry>
          <value>true</value>
          <label>ON</label>
        </entry>
        <entry>
          <value>false</value>
          <label>OFF</label>
        </entry>
      </entries>
      <selection-cardinality>1</selection-cardinality>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.use.gzip</name>
    <value>true</value>
    <description>
      Use GZip on archived logs (temporal cloud logs).
    </description>
    <display-name>Cloud Logs Use Gzip</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.on.shutdown</name>
    <value>false</value>
    <description>
      Rollover temporal cloud log files on shutdown
    </description>
    <display-name>Cloud Logs Rollover On Shutdown</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>logfeeder.cloud.rollover.on.startup</name>
    <value>true</value>
    <description>
      Rollover temporal cloud log files on startup
    </description>
    <display-name>Cloud Logs Rollover On Startup</display-name>
    <value-attributes>
      <type>boolean</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>

</configuration>
